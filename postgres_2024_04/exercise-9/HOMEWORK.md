# Работа с кластером высокой доступности

## Управлять кластером k8s планирую со своей основной машины на Linux, а подключаться попробую и с нее, и с клиентских ВМ в каждом регионе

### 1\. Проверяю статус установленного ранее kubectl

![e5d88c441aff735f5e4bc61a0c78839d.png](:/4443f9c75108459c9fd3c3f4b660c23f)

### 2\. Проверяю статус установленного ранее yc-cli

![cc078e73224aa262a278fcd27563216f.png](:/658b504e05684a64bead882de1845299)

### 3\. Подготавливаю инфраструктуру в YC

**1\. Инициализирую инфраструктуру**  

**2\. Выбираю дефолтный профиль, т.к. смысла создавать новый нет**  

**3\. Говорю использовать ранее заданный OAuth-токен**  

**4\. Хочу создать новую директорию для своих сервисов**  

**5\. Задаю ей имя**  

**6\. Отказываюсь от настройки Compute zone**  

**7\. Проверяю что я в только что созданной директории**  

![68cd0bc3700c19aa153bb5b6d3d02b16.png](:/a2f96d3c578f48528258b9f11d56d971)  

**Как это выглядит в интерфейсе:**  

Сервисы и ресурсы отсутствуют, как и должно быть  

![614060fd007b437a5e1260b5fef861a4.png](:/122ddda91f3e4d4aa59454d0e9d7af31)

### 4\. Создаю сервисный аккаунт и выдаю ему необходимые роли для управления кластером

![693281daa14bbf7df700b56776313873.png](:/9dcad0bf4c0f4145959ba0ce3ca20703)

### 5\. Создаю сеть и подсети в ней, для геораспределения узлов

![54eb3d18069a8b2901f723228cf69395.png](:/121d2eeb8568450286fdb4b269cb3ee1)

### 6\. На основе созданных объектов создаю кластер

![5f65e370e48a5872a4b1742b19bc1884.png](:/88551c1491784d5f80970bef2a6219f2)

### 7\. Проверяю статус кластера в CLI

![3ebdb27e7e9340aaac6e1463c7af92ec.png](:/1148b6c584614da8a0816790db8c2740)

### 8\. Создаю группы узлов

Убеждаюсь, что кластер в статусе RUNNING, иначе узлы создать не удастся  
![ee4f40e2a72d6c3b0b94c0ca9932f735.png](:/22094ef6a1c243aab2f6d3b7d55325e2)

### 9\. Создаю 2 группы узлов, одна для primary, другая для standby

**3 primary**, для отказоустойчивости  

**2 standby**, потому что ЯО больше ресурсов не захотело давать :D  

![9e1f99b8a76ad2c1dd2aa7acfcbdf154.png](:/d276c15e3db84da49840d9822cbe8725)

### 10\. Подключаю и проверяю статус узлов

![52482e2488943067c7af873d91a7c7ae.png](:/26209f6144364dab914a5e5677ef143d)

### 11\. Устанавливаю Helm для управления версионированием и разными версиями инсталляций

**Ставлю по [документации](https://helm.sh/ru/docs/intro/install/)**  
![ccb3a348ea4fac9b588bc8d662716df7.png](:/d7417996442242feb84eefd970906a22)

### 12\. Устанавливаю архив с Helm-чартом postgresql-ha

![0fd805e5ded9b43658b538f7c97d78cc.png](:/d4fe1ccd51fa4fceb3aab67862702418)

**В общем-то, получилось то, что проходили на практическом уроке. Но это pg-pool, это не то, к чему я шел изначально. Мне нужен Patroni**

## Иду делать Patroni! Для реализации мне понадобится утилита [Kind](https://kind.sigs.k8s.io/)

### 13. Устанавливаю Go-модуль Kind (Go у меня уже есть)

**1. Выполняю установку**

**2. Проверяю путь до установленного модуля**

**3. Пробую запустить, но мне это не удается**

**4. Редакатирую *~/.bashrc*, добавив alias для *kind***

**5. Логинюсь еще раз, для тестов**

**6. Сейчас в порядке**

![4447a4ce36c12519c0a5e672795b5132.png](:/912ed9ebdd494392b47713549e3b1598)

**Потом понимаю, что с sudo доступа все равно нет, и пришлось сделать симлинк в */usr/local/bin***

![c3715fb75f1531489da28c345cb1316c.png](:/802fe0688a014714a08345ca0c83ac21)

**Так лучше : )**

### 14. Создаю кластер *kind*

![6332184b15a2187f12ee0893a506f4d4.png](:/0d177d4e408f46e39592a59a69d9125f)

### 15. Пробую выводить информацию о кластере

![7ccfb1fc1bffbba662c8e703c1242aea.png](:/351e4984ce0d4456a4927e7c1e288edb)

### 16. Вытягиваю из Git'а пример кластера Patroni и выполняю подготовительные действия

**Пробую воспользоваться [инсталляцией Patroni](https://github.com/patroni/patroni/tree/master/kubernetes) для k8s**

**1. Для чистоты эксперимента удаляю сеществующий каталог и делаю *git clone***

**2. Вижу что есть *patroni_k8s.yaml*, кажется то что нужно**

**3. Иду его редактировать** 

**4. Меняю *image* на мой будущий docker-image и выставляю кол-во реплик: 5**

**5. Иду собирать образ *otus-patroni-k8s***

![ac782f63b5a7fe9f452fdfed70161082.png](:/1e8a82db523043728cef31e186d57713)

### 17. Пробую деплоить то что получилось

**1. Гружу только что собранный образ в инфраструктуру k8s**

**2. Пробую применить конфиг**

**3. Пробую получить список pod'ов**

![34e909e759f3abc7adf104bc3c76c436.png](:/003fc84e3ec74fe38c7dd4ab5a72edfc)

### 18. Проверяю работу кластера

![9e862b78c234e875ec1ac4348a68939b.png](:/b65bd202c3ce41508f4e05f373fdfd9e)

**Вроде бы все в порядке. Но так не очень удобно, поэтому пробую бросить порт на свою машину:**

![478629893725ad5c68028f6ba387edc8.png](:/a7580aa5810647d3aa7b4757a53d5114)

### 19. Эксперементирую

**1. Удаляю текущие pod'ы 0 и 1**

**2. Прверяю что они действительно удалились**

**3. Пробую подключиться к pod'е 3**

**4. Вижу, что лидером стала нода 4,  и  БД сохранилась, отлично**

![a3a2f376cafa939dba3491eb4236a33e.png](:/0871f894752e4fe5bd7e1cb1d191ec23)
